#include <mpi.h>
#include <random>
#include "../../../modules/task_1/lbov_a_sum_elements_matrix/sum_element_matrix.h"

int* generate_matrix(int size) { //Создание матрицы и её заполнение
    std::random_device dev;
    std::mt19937 gen(dev());
    int* matrix = new int[size];
    for (int i = 0; i < size; i++) {
        matrix[i] = gen() % 100;
    }
    return matrix;
}

int not_parallel_sum(int* matrix, int size) { //обычная функция суммы элементов в массиве
    int s = 0;
    for (int i = 0; i < size; i++)
        s += matrix[i];
    return s;
}

int parallel_sum(int* matrix, int size) {
    int lsize, rank;
    MPI_Comm_size(MPI_COMM_WORLD, &lsize); 
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
     const int okr = size / lsize; // по рекомендации Сысоева(стр.39) необходимы для равного распределния элементов между процессами
    const int ost = size % lsize;

    if (rank == 0) {
       // MPI_Bcast(matrix, okr, MPI_INT, 0, MPI_COMM_WORLD);// Сысоев говорил использовать Bcast, но он не рабоает почему то(
        for (int process = 1; process < lsize; process++) {
            MPI_Send(matrix + process * okr + ost, okr, MPI_INT, process, 0, MPI_COMM_WORLD);
        }
    }
     // Идёт рассылка всем процессам в главном камутаторе
   
    

    int* l_matrix = nullptr; // создание локальной матрицы для работы с процессами
    if (rank == 0) {
        l_matrix = new int[ost + okr];
    }
    else {
        l_matrix = new int[okr];
    }
    
    if (rank == 0) {
        for (int i = 0; i < ost + okr; i++) {
            l_matrix[i] = matrix[i];
        }
    }
    else {
        MPI_Status status;
        MPI_Recv(l_matrix, okr, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);
    }

    int sum = 0;
    int l_sum = 0;

    if (rank == 0) {
        l_sum = not_parallel_sum(l_matrix, ost + okr);
    }
    else {
        l_sum = not_parallel_sum(l_matrix, okr);
    }

    MPI_Reduce(&l_sum, &sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
    return sum;





}